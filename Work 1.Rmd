---
title: "Work 1"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r message=FALSE}
library(MVA)
```

# Problem 1
Answer the following questions:

1) How many columns are in a 2$\times$n matrix? How many rows? Give an example in R.
Here there are (n) number of columns in the matrix and 2 rows.
so we will take a random value of n i.e. n=4.
```{r}
n <- 4  
matrix(c(2*n),nrow = 2,ncol = n)

```

2) $\begin{bmatrix} 2 & -1 \end{bmatrix}\begin{bmatrix} 1 & 1 \\ 2 & 0 \end{bmatrix}$ = ? (Explain and answer in R)
```{r}
X<-matrix(c(2,-1),ncol = 2)
X
Y<-matrix(c(1,1,2,0),ncol=2,byrow=TRUE)
Y
C<-X%*%Y
C
```

3) $\begin{bmatrix} 1 & 0 \\ 2 & 0 \end{bmatrix}\begin{bmatrix} 2 & -1 \end{bmatrix}$ = ? (Explain and answer in R) 
```{r}
A<-matrix(c(1,0,2,0),ncol=2,byrow=TRUE)
A
B<-matrix(c(2,-1),ncol = 2)
B
C<-B%*%A
C
```

4) Given $A = \begin{bmatrix} 1 & 0 \\ 2 & 0 \end{bmatrix}$, find the result of the multiplication: $(A)(A^{-1})$ in R. Explain briefly. 
#Matrix A is a singular matrix in this case as its determinant is zero. 
#We are unable to obtain matrix A's inverse.
#The determinant needs to be non-zero in order to derive matrix A's inverse.
```{r}
A<-matrix(c(1,0,2,0),ncol=2,byrow=TRUE)
A

detA=det(A)
detA

#A_inv <- solve(A)
#A_inv

```

# Problem 2
Convert this covariance matrix into the corresponding correlation matrix (using R). Validate one of the correlatins by hand (using the correlation formula). 

$$\left[\begin{array}{cccc}
                     3.877 & 2.811 & 3.148 & 3.506 \\
                     2.811 & 2.121 & 2.266 & 2.569 \\
                     3.148 & 2.266 & 2.655 & 2.834 \\
                     3.506 & 2.569 & 2.834 & 3.235
\end{array}\right]$$

```{r}
# Define the covariance matrix
cov_matrix <- matrix(c(
  3.877, 2.811, 3.148, 3.506,
  2.811, 2.121, 2.266, 2.569,
  3.148, 2.266, 2.655, 2.834,
  3.506, 2.569, 2.834, 3.235
), nrow = 4, ncol = 4)

# covariance matrix to correlation matrix
cor_matrix <- cov2cor(cov_matrix)

#  The correlation matrix is
print(cor_matrix,2)

#To compute the correlation between the first and third columns, 
#divide the covariance between the first and third columns by the 
#product of the standard deviations of the first and second variables.

cov_matrix[2,4]/(sqrt(diag(cov_matrix)[2])*sqrt(diag(cov_matrix)[4]))
```

# Problem 3
Report the euclidean distance matrix for the first 10 cities (rows of the data). What cities are the most similar to each other in that list? Why?

#Ans
#Columbus is near to Albany
#Columbus is near to Albuquerque
#Baltimore is near to Atlanta
#Columbus is near to Baltimore
#Cleveland is near to Buffalo
#Cincinnati is near to Charleston
#Cleveland is near to Chicago
#Columbus is near to Cincinnati , from this we can conclude that these cities are similar to each other.

```{r}
data("USairpollution", package = "HSAUR2")
mydata <- USairpollution[1:10,]

USairpollution.u=scale(USairpollution[1:10, ])

d.s=dist(USairpollution.u)
round(d.s, 2)

```

# Problem 4
Use the bivariate boxplot on the scatterplot of pairs of variables ((temp, wind), (temp, precip), (temp, predays)) in the air pollution data to identify any outliers. Calculate the correlation between each pair of variables using all the data and the data with any identified outliers removed. Comment on the results.

```{r}
data("USairpollution", package = "HSAUR2")
library(MVA)
data("USairpollution", package = "HSAUR2")
data<-USairpollution[,c("temp","wind")]
head(data)

a <-data
xbar <- colMeans(a)

s <- cov(a)

round(s,2)

d2 <- mahalanobis(a, xbar, s)
head(d2)

numberVariables <-ncol(a)
Position = (1:nrow(a) - 1/2)/nrow(a)
quantiles <- qchisq(Position, df = numberVariables)

plot(quantiles, sort(d2),
     xlab = expression(paste(chi[2]^2, " Quantile")),
     ylab = "Ordered squared distances")
abline(a = 0, b = 1)
text(quantiles, sort(d2), names(sort(d2)), col = "blue")

lab <- c("Miami","Phoenix")

Places <- match(lab, row.names(data))

bvbox(data, xlab = "temp ", ylab = "wind",col = "red", cex = 0.5)
text(data$temp[Places], data$wind[Places], labels = lab,
     cex = 0.7, pos = c(2, 3, 3, 3, 4))

corr <- cor(data$temp,data$wind)
round(corr,2)

corr.no_Outlier <- cor(data$temp[-Places],data$wind[-Places])
round(corr.no_Outlier,2)

#The temperature and wind association rose when the outliers were eliminated.

data("USairpollution", package = "HSAUR2")
data1 <- USairpollution[, c("temp", "precip")]
head(data1)

a <-data1

xbar <- colMeans(a)

s <- cov(a)
round(s,2)

d2 <- mahalanobis(a, xbar, s)
head(d2)


numberVariables <-ncol(a)

Position = (1:nrow(a) - 1/2)/nrow(a)

quantiles <- qchisq(Position, df = numberVariables)

plot(quantiles, sort(d2),
     xlab = expression(paste(chi[2]^2, " Quantile")),
     ylab = "Ordered squared distances")
abline(a = 0, b = 1)
text(quantiles, sort(d2), names(sort(d2)), col = "green")

lab <- c("Miami","Phoenix", "Albuquerque")


Places1 <- match(lab, row.names(data1))

bvbox(data1, xlab = "temp", ylab = "precip",col = "red", cex = 0.5)
text(data1$temp[Places1], data1$precip[Places1], labels = lab,
     cex = 0.7, pos = c(2, 3, 3, 3, 4))

corr <- cor(data1$temp,data1$precip)
round(corr,2)

corr.no_Outlier <- cor(data1$temp[-Places1], data1$precip[-Places1])
round(corr.no_Outlier,2)

#The temp and precip association rose when the outliers were eliminated.

data("USairpollution", package = "HSAUR2")
mydata2 <- USairpollution[, c("temp", "predays")]
head(mydata2)
x <-mydata2
xbar <- colMeans(x)
s <- cov(x)
round(s,2)

d2 <- mahalanobis(x, xbar, s)
head(d2)

numberVariables <-ncol(x)

Position = (1:nrow(x) - 1/2)/nrow(x)

quantiles <- qchisq(Position, df = numberVariables)

plot(quantiles, sort(d2),
     xlab = expression(paste(chi[2]^2, " Quantile")),
     ylab = "Ordered squared distances")
abline(a = 0, b = 1)
text(quantiles, sort(d2), names(sort(d2)), col = "red")

lab <- c("Miami","Phoenix" )

Cities2 <- match(lab, row.names(mydata2))

bvbox(mydata2, xlab = "temp in celsius", ylab = "predays",col = "red", cex = 0.5)
text(mydata2$temp[Cities2], mydata2$predays[Cities2], labels = lab,
     cex = 0.7, pos = c(2, 3, 3, 3, 4))



corr <- cor(mydata2$temp, mydata2$predays)
round(corr,2)

corr.no_Outlier <- cor(mydata2$temp[-Cities2], mydata2$predays[-Cities2])
round(corr.no_Outlier,2)

#The association between temperature and predays somewhat increased 
#once the outliers were eliminated.

```

# Problem 5
Examine the multivariate normality of the pottery data by creating the chi-square plot of the Mahalanobis distances. The data gives the chemical composition of specimens of Romano-British pottery. The variable "kiln" is categorical representing site at which the pottery was found. You may consider transforming the categorical variable into dummy variables before doing the analysis. 

Load the data as the following.

```{r}
library(MVA)

data(pottery, package = "HSAUR2")
mydata <- pottery

#install.packages('fastDummies')
library(fastDummies)

mydata_n<- dummy_cols(mydata,select_columns=c("kiln"),
                  remove_selected=TRUE,remove_first_dummy=TRUE)
x1<-mydata_n
xbar <- colMeans(x1)
S <- cov(x1)

d2 <- mahalanobis(x1, xbar, S)
head(d2)

numberVariables <-ncol(x1)

Position = (1:nrow(x1) - 1/2)/nrow(x1)

quantiles <- qchisq(Position, df = numberVariables)

plot(quantiles, sort(d2),
     xlab = expression(paste(chi[13]^2, " Quantile")),
     ylab = "Ordered squared distances")
abline(a = 0, b = 1)
```


# Problem 6
The banknote dataset contains measurements on 200 Swiss banknotes: 100 genuine and 100 counterfeit. The variables are length of bill, width of left edge, width of right edge , bottom margin width and top margin width. All measurements are in millimeters. The data source is noted below. This data is available in the "mclust" package in R. Pick the variables "Bottom" and "Diagonal". Please work on the following:

a) Construct univariate kernel estimates (Gaussian kernel) of the distributions of these two variables (two graphs). Experiment with bandwidths (actual numbers, not defaults) to get nice-looking graphs. 

#ans- We have estimated univariate kernel of Bottom and Diagonal variables.
# also we used different bandwidths,bw=0.6 and bw=0.30
```{r}
library(mclust)
data(banknote, package = "mclust")
mydata <- banknote[,c(1,5,7)]

hist(mydata$Bottom)
hist(mydata$Diagonal)

density_calculate1<-(density(mydata$Bottom))
density_calculate2<-(density(mydata$Diagonal))

plot(density_calculate1)
plot(density_calculate2)

plot(density(mydata$Bottom,bw=0.6,kernel="gaussian"))
plot(density(mydata$Bottom,bw=0.6,kernel="gaussian"))

plot(density(mydata$Bottom,bw=0.30,kernel="gaussian"))
plot(density(mydata$Bottom,bw=0.30,kernel="gaussian"))
```

b) Using the bivariate Gaussian kernel, estimate the bivariate density of the two variables using (i) a contour plot and (ii) a 3-D perspective plot. Use the bandwidths you finally chose in part a.

#ans-We have use bandwith 0.30 as per above question and the estimated the 
#estimate the bivariate density of the two variables using both the options
#(i) & (ii).
```{r}
#install.packages("HSAUR")
data("CYGOB1", package = "HSAUR")
library("KernSmooth")
library(fastDummies)
library(mclust)
data(banknote,package="mclust")
mydata <- banknote[,c(5,7)]

bw<-c(0.30,0.30)

density<-bkde2D(mydata,bandwidth=bw)


plot(mydata,xlab="Bottom",ylab = "Diagonal",main="lenght of bill")

contour(x=density$x1, y=density$x2 , z=density$fhat,add=TRUE)


persp(x = density$x1, y = density$x2,
      z = density$fhat,
      xlab = "Bottom",
      ylab = "Diagonal",
      zlab = "density",
      main="Perspective Plot of Swiss Banknote", phi = 30)
```

c) Plot the scatterplot, highlighting points with different colors according to whether the bills are genuine or counterfeit (the "Status" variable in the data set has that information).
#ANS-Red data points are genuine and Black dots are counterfeit.
```{r message=FALSE}
# install.packages("mclust")
library(mclust)
data(banknote, package = "mclust")
mydata <- banknote[,c(1,5,7)]
plot(mydata$Bottom,mydata$Diagonal,col=mydata$Status)
```

